{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d548358",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a4cf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\conta\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\conta\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification,AutoTokenizer,pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679eba5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\conta\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "model=TFAutoModelForSequenceClassification.from_pretrained(model_name,from_pt=True)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "classifier=pipeline('sentiment-analysis',model=model,tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb298289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.7982096672058105}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(\"Very bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c08db",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470756d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=pipeline('text-generation',model='distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06763102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I like to'},\n",
       " {'generated_text': 'I like to'},\n",
       " {'generated_text': 'I like a'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"I like\",max_length=3,num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388520e",
   "metadata": {},
   "source": [
    "# Zero-text-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069215b",
   "metadata": {},
   "source": [
    "In Hugging Face Transformers, zero-shot text classification refers to the ability of a model to classify text into predefined categories without being explicitly trained on examples from those categories. This is a powerful capability as it allows you to leverage existing models for new classification tasks without the need for additional labeled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d20ef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "classifier=pipeline('zero-shot-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8355aa6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I make students learn ',\n",
       " 'labels': ['teacher', 'security guard'],\n",
       " 'scores': [0.9903699159622192, 0.009630076587200165]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I make students learn \",candidate_labels=[\"teacher\",\"security guard\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503339b7",
   "metadata": {},
   "source": [
    "## Fill mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d341ab36",
   "metadata": {},
   "source": [
    "It involves training the model to predict a masked word based on the surrounding context in a sentence. Here's a detailed explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b0e658",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.034582920372486115,\n",
       "  'token': 14415,\n",
       "  'token_str': ' NYC',\n",
       "  'sequence': 'I live in NYC and i want to be a data scientist'},\n",
       " {'score': 0.024085989221930504,\n",
       "  'token': 20071,\n",
       "  'token_str': ' Bangalore',\n",
       "  'sequence': 'I live in Bangalore and i want to be a data scientist'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker=pipeline(\"fill-mask\")\n",
    "unmasker(\"I live in <mask> and i want to be a data scientist\",top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc4f825",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a5565b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Transformers are a powerful neural network architecture renowned for their exceptional performance on a wide range of tasks . Transformers essentially consist of two sub-networks: Encoder-Decoder Structure and Decoder Structure . Transformers have consistently achieved state-of-the-art results on various NLP tasks, including text classification, machine translation and question answering .'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer=pipeline(\"summarization\")\n",
    "summarizer(\"\"\"In the realm of natural language processing (NLP), transformers are a powerful neural network architecture renowned for their exceptional performance on a wide range of tasks. Here's a comprehensive breakdown of their key characteristics and applications:\n",
    "\n",
    "Core Components:\n",
    "\n",
    "Encoder-Decoder Structure: Transformers essentially consist of two sub-networks:\n",
    "\n",
    "Encoder: This sub-network processes the input text, capturing its meaning and the relationships between words. It often utilizes multiple stacked encoder layers.\n",
    "Decoder: This sub-network generates the output text, conditioned on the encoded representation from the encoder.\n",
    "Self-Attention Mechanism: A key innovation in transformers is the self-attention mechanism. Unlike traditional recurrent neural networks (RNNs) that process sequences one element at a time, self-attention allows transformers to:\n",
    "\n",
    "Focus on important parts of the input sequence: The model can attend to relevant words in the entire sentence simultaneously, not just the previous word.\n",
    "Learn long-range dependencies: This mechanism enables the model to capture relationships between words that might be far apart in the sentence, a limitation faced by RNNs.\n",
    "Benefits of Transformers:\n",
    "\n",
    "Superior Performance: Transformers have consistently achieved state-of-the-art results on various NLP tasks, including:\n",
    "\n",
    "Text classification (sentiment analysis, topic labeling)\n",
    "Question answering\n",
    "Machine translation\n",
    "Text summarization\n",
    "Text generation\n",
    "Parallelization Potential: Due to their ability to process the entire input sequence at once, transformers have the potential for parallelization, leading to faster training compared to RNNs.\n",
    "\n",
    "Applications:\n",
    "\n",
    "Chatbots and Virtual Assistants: Transformers power the ability of chatbots and virtual assistants to understand complex questions and provide informative responses.\n",
    "Machine Translation: Transformers have revolutionized machine translation, enabling more accurate and nuanced translations between languages.\n",
    "Text Summarization: Transformers can effectively condense lengthy text into concise summaries while preserving key information.\n",
    "Content Creation: They are used in applications like generating creative text formats, marketing copy, or different writing styles.\n",
    "Example:\n",
    "\n",
    "Consider the sentence \"The quick brown fox jumps over the lazy dog.\" In traditional RNNs, the network processes words sequentially, potentially missing long-range dependencies like the relationship between \"fox\" and \"jumps\" (which are not directly next to each other). However, transformers can attend to the entire sentence at once, capturing these crucial connections.\n",
    "\n",
    "Overall, transformers represent a significant advancement in NLP, offering superior performance, versatility, and the potential for faster training. Their ability to effectively capture long-range dependencies and contextual information makes them a cornerstone of modern NLP applications.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e29e0d8",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4d870a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.48041731119155884,\n",
       " 'start': 7,\n",
       " 'end': 26,\n",
       " 'answer': 'engineering student'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answer=pipeline(\"question-answering\")\n",
    "question_answer(question=\"What is my age?\",context=\"Iam an engineering student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb6a9a8",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4134d6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\conta\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Hello. How are you?'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "trans= pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "trans(\"你好你好吗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a194f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
