{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4abaaf8a",
   "metadata": {},
   "source": [
    "# Pre Processing techniques for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa0cc89",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ec033",
   "metadata": {},
   "source": [
    "The words which are generally filtered out out before processing a natural language are called stop words .\n",
    "These are the words which do not carry meaningful information about the content of the text.\n",
    "Stop words are used to remove noise from the data and speed up the computation process.\n",
    "Ex:the\", \"is\", \"at\", \"which\", \"and\", \"on\", \"in\", \"of\", \"to\", etc.,  we can even include punctuations here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302acc17",
   "metadata": {},
   "source": [
    "To perform the stop words operation, we will use the NLTK library. NLTK stands for Natural Language Toolkit. It is a leading platform for building Python programs to work with human language data, particularly in the field of natural language processing (NLP). NLTK provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, among other NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04a6b785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\conta\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b2b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example sentence to perform stop words operation\n",
    "sentence=\"\"\"NLTK is a leading platform for building Python \n",
    "programs to work with human language data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e0d3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words(\"english\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061fc721",
   "metadata": {},
   "source": [
    "nltk.download('stopwords') run this code if the above code is throwing an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1ea7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "#Example stop words\n",
    "print(stop_words[0:20]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f5401bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc=list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a12f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d928354",
   "metadata": {},
   "source": [
    "Now we have added all the stop words and unnecessary punctuation marks in the stop_words variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ccf782",
   "metadata": {},
   "source": [
    "Now we will be transforming sentence into lower case and splitting it because the stop words is in lower case and in form of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d923aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_splitted=sentence.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba27ddaa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk',\n",
       " 'is',\n",
       " 'a',\n",
       " 'leading',\n",
       " 'platform',\n",
       " 'for',\n",
       " 'building',\n",
       " 'python',\n",
       " 'programs',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " 'human',\n",
       " 'language',\n",
       " 'data.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d993c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_without_stopwords=[word for word in sentence_splitted if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e16f4b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f067a6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_splitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50bd777",
   "metadata": {},
   "source": [
    "Now join the sentence without stop words and compare it with original sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56fc39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullsentence_without_stopwords=' '.join(sentence_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23f47456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nltk leading platform building python programs work human language data.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullsentence_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37600573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLTK is a leading platform for building Python \\nprograms to work with human language data.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6e123",
   "metadata": {},
   "source": [
    "We can see the difference between the original sentence and the updated sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4953d",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec0e0c",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), tokens are the individual units of text that make up a larger body of text. These units can vary depending on the level of granularity required for the task at hand. Tokenization is the process of breaking down text into these smaller units.\n",
    "\n",
    "Here are some common types of tokens in NLP:\n",
    "\n",
    "1. **Word Tokens**: These are tokens representing individual words in the text.\n",
    "\n",
    "2. **Character Tokens**: These tokens represent individual characters in the text.\n",
    "\n",
    "3. **Subword Tokens**: These tokens represent smaller linguistic units, such as prefixes, suffixes, or roots of words. Subword tokenization is often used to handle out-of-vocabulary words and improve the generalization of models.\n",
    "\n",
    "4. **Special Tokens**: These are tokens that serve specific purposes in certain models or tasks. They are not typically part of the input text but are added during preprocessing or model training to convey additional information.\n",
    "\n",
    "Now, let's take the example of BERT (Bidirectional Encoder Representations from Transformers), one of the most popular models used in NLP. BERT utilizes several special tokens:\n",
    "\n",
    "1. **[CLS]** (Classification Token): In BERT, the input to the model consists of a sequence of tokens. BERT adds a special [CLS] token at the beginning of every input sequence. This token is used to represent the sequence as a whole for tasks such as sentence classification or sentence pair classification.\n",
    "\n",
    "2. **[SEP]** (Separator Token): BERT uses the [SEP] token to separate pairs of sentences in tasks such as question answering or natural language inference. It indicates the end of one sentence and the beginning of another.\n",
    "\n",
    "3. **[MASK]** (Mask Token): During pretraining, BERT employs the [MASK] token to mask certain tokens in the input sequence. The model is then trained to predict these masked tokens based on the context provided by the surrounding tokens.\n",
    "\n",
    "4. **[PAD]** (Padding Token): In order to process sequences of varying lengths efficiently, BERT pads shorter sequences with [PAD] tokens to make them equal in length to the longest sequence in the batch.\n",
    "\n",
    "These special tokens play crucial roles in how BERT processes and understands text, enabling it to perform tasks such as text classification, named entity recognition, and question answering effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39c7a3",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7005b0c6",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down a text or a sequence of characters into smaller units called tokens. These tokens can be words, subwords, or characters, depending on the granularity required for the task at hand. Tokenization is a fundamental step in natural language processing (NLP) and is essential for tasks such as text analysis, sentiment analysis, language modeling, and machine translation.\n",
    "\n",
    "Here are the common types of tokenization:\n",
    "\n",
    "Word Tokenization: This type of tokenization splits the text into words based on space or punctuation boundaries. For example, the sentence \"Tokenization is important in NLP.\" would be tokenized into [\"Tokenization\", \"is\", \"important\", \"in\", \"NLP\"].\n",
    "\n",
    "Sentence Tokenization: Sentence tokenization involves splitting the text into individual sentences. This is often done by identifying punctuation marks (such as periods, exclamation marks, or question marks) that denote the end of a sentence. For example, the paragraph \"This is the first sentence. This is the second sentence!\" would be tokenized into [\"This is the first sentence.\", \"This is the second sentence!\"].z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dd7c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e3516e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"\"\"The sun sets behind the horizon,\n",
    "Painting the sky with hues of orange and pink.\n",
    "As darkness falls, stars twinkle in the night sky.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6420e34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sun sets behind the horizon,\\nPainting the sky with hues of orange and pink.\\nAs darkness falls, stars twinkle in the night sky.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1adbca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  nltk.download('punkt')\n",
    "sentence_tokenized=sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b09284e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The sun sets behind the horizon,\\nPainting the sky with hues of orange and pink.',\n",
       " 'As darkness falls, stars twinkle in the night sky.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84a77407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun sets behind the horizon,\n",
      "Painting the sky with hues of orange and pink.\n",
      "As darkness falls, stars twinkle in the night sky.\n"
     ]
    }
   ],
   "source": [
    "for sent in sentence_tokenized:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb43fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenized=word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03899490",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'sun',\n",
       " 'sets',\n",
       " 'behind',\n",
       " 'the',\n",
       " 'horizon',\n",
       " ',',\n",
       " 'Painting',\n",
       " 'the',\n",
       " 'sky',\n",
       " 'with',\n",
       " 'hues',\n",
       " 'of',\n",
       " 'orange',\n",
       " 'and',\n",
       " 'pink',\n",
       " '.',\n",
       " 'As',\n",
       " 'darkness',\n",
       " 'falls',\n",
       " ',',\n",
       " 'stars',\n",
       " 'twinkle',\n",
       " 'in',\n",
       " 'the',\n",
       " 'night',\n",
       " 'sky',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd76361",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51de9c",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), stemming is the process of reducing words to their root or base form, also known as the stem. The main goal of stemming is to reduce inflected words to their common base form, which can help improve text analysis and information retrieval tasks by treating different forms of a word as the same entity.\n",
    "\n",
    "For example, stemming would convert words like \"running\", \"runs\", and \"ran\" to the common base form \"run\". Similarly, words like \"play\", \"playing\", and \"played\" would all be stemmed to \"play\".\n",
    "\n",
    "Stemming algorithms typically work by removing suffixes from words to obtain the root form. These algorithms are rule-based and operate by applying a series of rules to trim off common suffixes. However, stemming algorithms do not always produce accurate or linguistically valid results, as they may sometimes produce stems that are not actual words or may result in stems that are not semantically related.\n",
    "\n",
    "Despite its limitations, stemming is still widely used in NLP tasks such as text normalization, information retrieval, and document clustering. It can help reduce the dimensionality of text data and improve the performance of certain text processing tasks. Popular stemming algorithms include the Porter Stemmer and the Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e16c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,LancasterStemmer,RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "667fabe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter=PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b11f230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chang\n",
      "chang\n",
      "chang\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem(\"changing\"))\n",
    "print(porter.stem(\"changed\"))\n",
    "print(porter.stem(\"change\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8e82207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chang\n",
      "chang\n",
      "chang\n"
     ]
    }
   ],
   "source": [
    "#Set of rules to change are different\n",
    "print(lancaster.stem(\"changed\"))\n",
    "print(lancaster.stem(\"changed\"))\n",
    "print(lancaster.stem(\"change\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5003ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"changed\",\"changing\",\"change\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a817024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chang', 'chang', 'chang']\n"
     ]
    }
   ],
   "source": [
    "port_stemmer=[porter.stem(word) for word in words]\n",
    "print(port_stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58b2a4",
   "metadata": {},
   "source": [
    "## Lemmetization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b298d8a1",
   "metadata": {},
   "source": [
    "\n",
    "Lemmatization, like stemming, is a natural language processing (NLP) technique used to reduce words to their base or dictionary form, known as the lemma. However, unlike stemming, lemmatization considers the context and meaning of a word when determining its lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62e46d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01428c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10987abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmetizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33512680",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_words=[lemmetizer.lemmatize(word,wordnet.VERB) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56ecf333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['change', 'change', 'change']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95e7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
